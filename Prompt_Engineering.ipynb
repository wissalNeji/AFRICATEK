{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz8qxmpkz7KA"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the file into a DataFrame\n",
        "df = pd.read_csv('healthcare_messy_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "o_K873Z70QSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=\"ENTER_YOUR_TOKEN\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", use_auth_token=\"ENTER_YOUR_TOKEN\")"
      ],
      "metadata": {
        "id": "0FIVf6mh0Q-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model identifier on Hugging Face\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"  # Make sure you accept the terms of use\n",
        "\n",
        "# Loading tokenizer and template\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Creation of the generation pipeline\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "print(\"âœ… LLaMA model successfully loaded!\")"
      ],
      "metadata": {
        "id": "jXBbqUo00W9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a text prompt to send to the model\n",
        "# Extract key statistics\n",
        "rows = len(df)\n",
        "columns = len(df.columns)\n",
        "missing_fixed = df.isnull().sum().sum()\n",
        "summary_stats = df.describe().to_dict()  # converts numerical stats into a dictionary\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Below is a summary of key statistics from the dataset:\n",
        "- Total rows: {rows}\n",
        "- Total columns: {columns}\n",
        "- Number of missing values fixed: {missing_fixed}\n",
        "\n",
        "Please give three key insights in bullet form, under 100 words.\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "TBrKSxaH0ZTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text generation with LLaMA\n",
        "output = llama_pipeline(prompt, max_new_tokens=150, temperature=0.7)\n",
        "\n",
        "# Display generated summary\n",
        "print(\"Summary generated by LLaMA :\\n\", output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "AX5F7Z_t0dHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary stats\n",
        "summary_stats = df.describe().transpose()\n",
        "\n",
        "# Convert stats to string\n",
        "stats_text = summary_stats.to_string()\n",
        "prompt = f\"\"\"\n",
        "You are a data analyst. Analyze the following dataset summary and provide a short report on key trends, outliers, or correlations.\n",
        "\n",
        "- Total rows: {rows}\n",
        "- Total columns: {columns}\n",
        "- Number of missing values fixed: {missing_fixed}\n",
        "\n",
        "Statistical Summary:\n",
        "{stats_text}\n",
        "\n",
        "Write your analysis in a concise and professional tone.\n",
        "\"\"\"\n",
        "\n",
        "output = llama_pipeline(prompt, max_new_tokens=300, temperature=0.7)\n",
        "\n",
        "# Display the result\n",
        "print(\"Generated summary by LLaMA:\\n\", output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "2jPWNopC0ipJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s9ZyzgMQe973"
      }
    }
  ]
}